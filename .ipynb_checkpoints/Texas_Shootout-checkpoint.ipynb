{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading train.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv', header=None, skiprows=1, parse_dates=[0,11,12])\n",
    "print(\"finished reading train.csv\")\n",
    "data.columns = [\"date_time\", \"site_name\", \"posa_continent\", \"user_location_country\",\n",
    "                \"user_location_region\", \"user_location_city\", \"orig_destination_distance\",\n",
    "                \"user_id\", \"is_mobile\", \"is_package\", \"channel\", \"srch_ci\", \"srch_co\",\n",
    "                \"srch_adults_cnt\", \"srch_children_cnt\", \"srch_rm_cnt\", \"srch_destination_id\",\n",
    "                \"srch_destination_type_id\", \"is_booking\", \"cnt\", \"hotel_continent\", \"hotel_country\",\n",
    "                \"hotel_market\", \"hotel_cluster\"] \n",
    "data.drop(data.index[[406250,1373714,1373715,2374706,3504146,3707429,7780825,778086,8040327,8428261,8428262,\n",
    "                     15336804,16428469,16695744,17669511,18156670,18575457,31775817,31784369,32146102,32168215,32549132,\n",
    "                     34815966,34815968,34815969,35412891,36606126,36833435,36833436,37330211]], inplace=True)\n",
    "#data = data.loc[data['is_booking'] == 1] # contains only booking events\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data loaded\n",
      "test data loaded\n",
      "pp part 1 complete\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 16 21:32:03 2016\n",
    "\n",
    "@author: Michael\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"from ml_metrics import average_precision\"\n",
    "\n",
    "\n",
    "print(\"training data loaded\")\n",
    "#dest = pd.read_csv('destinations_mod.csv')\n",
    "#print(\"dest data loaded\")\n",
    "test = pd.read_csv('test.csv', parse_dates=['date_time', 'srch_co'])\n",
    "print(\"test data loaded\")\n",
    "test = test.drop('id', 1)\n",
    "   \n",
    "data = data.drop('cnt', 1)\n",
    "data = data.drop('is_booking', 1)\n",
    "data = data.drop('site_name', 1)\n",
    "data = data.drop('posa_continent', 1)\n",
    "data = data.drop('is_mobile', 1)\n",
    "data = data.drop('is_package', 1)\n",
    "data = data.drop('srch_adults_cnt', 1)\n",
    "data = data.drop('srch_children_cnt', 1)\n",
    "data = data.drop('srch_rm_cnt', 1)\n",
    "data = data.drop('srch_destination_type_id', 1)\n",
    "data = data.drop('hotel_continent', 1)\n",
    "data_y = data[\"hotel_cluster\"]\n",
    "data = data.drop('hotel_cluster', 1)\n",
    "data['srch_ci'] = pd.to_datetime(data['srch_ci'], errors='coerce')\n",
    "data['srch_ci'].fillna(value=pd.to_datetime('7/1/2014'), inplace=True)\n",
    "data['srch_co'] = pd.to_datetime(data['srch_co'], errors='coerce')\n",
    "data['srch_co'].fillna(value=pd.to_datetime('7/4/2014'), inplace=True)\n",
    "data['orig_destination_distance'].fillna(1969, inplace=True)\n",
    "test = test.drop('site_name', 1)\n",
    "test = test.drop('posa_continent', 1)\n",
    "test = test.drop('is_mobile', 1)\n",
    "test = test.drop('is_package', 1)\n",
    "test = test.drop('srch_adults_cnt', 1)\n",
    "test = test.drop('srch_children_cnt', 1)\n",
    "test = test.drop('srch_rm_cnt', 1)\n",
    "test = test.drop('srch_destination_type_id', 1)\n",
    "test = test.drop('hotel_continent', 1)\n",
    "test['srch_ci'].replace(to_replace='nan', value=pd.to_datetime('1/1/2017'), inplace=True)\n",
    "test.set_value(312920, 'srch_ci', pd.to_datetime('1/21/2016'))\n",
    "test['srch_ci'] = pd.to_datetime(test['srch_ci'], errors='coerce')\n",
    "test['srch_co'] = pd.to_datetime(test['srch_co'], errors='coerce')\n",
    "test['srch_ci'].fillna(value=pd.to_datetime('7/1/2015'), inplace=True)\n",
    "test['srch_co'].fillna(value=pd.to_datetime('7/3/2015'), inplace=True)\n",
    "test['orig_destination_distance'].fillna(1685, inplace=True)\n",
    "test_bookings = test.as_matrix(['srch_ci','srch_co'])\n",
    "train_bookings = data.as_matrix(['srch_ci','srch_co'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"pp part 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of stay values appended\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import re\n",
    "def addStayLengths(matrix_input): #takes in numpy matrices and returns \n",
    "    length_of_stay_values = []\n",
    "    for i in range(0, matrix_input.shape[0]):\n",
    "        m1 = re.search(r'(\\d+-\\d+-\\d+)', str(matrix_input[i][0])) #srch_ci\n",
    "        m2 = re.search(r'(\\d+-\\d+-\\d+)', str(matrix_input[i][1])) #srch_co\n",
    "        match1 = m1.group(0)\n",
    "        values1 = match1.split('-')\n",
    "        year1 = int(values1[0])\n",
    "        month1 = int(values1[1])\n",
    "        day1 = int(values1[2]) \n",
    "\n",
    "        match2 = m2.group(0)\n",
    "        values2 = match2.split('-')         \n",
    "        year2 = int(values2[0])\n",
    "        month2 = int(values2[1])\n",
    "        day2 = int(values2[2])     \n",
    "    #try:\n",
    "        d0 = date(year1, month1, day1)\n",
    "        d1 = date(year2, month2, day2)\n",
    "        delta = d1 - d0\n",
    "        length_of_stay_values.append(delta.days)\n",
    "    #except:\n",
    "    #    print(\"exception: \" + str(matrix_input[i][0]) + \", \" + str(matrix_input[i][1]))\n",
    "    #    length_of_stay_values.append(3)\n",
    "    return length_of_stay_values\n",
    "traincolumn = addStayLengths(train_bookings)\n",
    "testcolumn = addStayLengths(test_bookings)\n",
    "data = data.assign(length_of_stay = traincolumn)\n",
    "test = test.assign(length_of_stay = testcolumn)\n",
    "print(\"length of stay values appended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new features added\n",
      "pp part 2 complete\n",
      "new features added\n",
      "pp part 2 complete\n"
     ]
    }
   ],
   "source": [
    "def addFeatures(df_input):\n",
    "    df_input['date_time_month'] = df_input['date_time'].dt.month\n",
    "    df_input['date_time_day'] = df_input['date_time'].dt.day\n",
    "    df_input['date_time_hour'] = df_input['date_time'].dt.hour\n",
    "    df_input['srch_ci_month'] = df_input['srch_ci'].dt.month\n",
    "    df_input['srch_ci_dayofweek'] = df_input['srch_ci'].dt.dayofweek\n",
    "    df_input['srch_ci_day'] = df_input['srch_ci'].dt.day\n",
    "    #df_input['srch_co_month'] = df_input['srch_co'].dt.month\n",
    "    print(\"new features added\")\n",
    "    df_input = df_input.drop('date_time', 1)\n",
    "    df_input = df_input.drop('srch_ci', 1)\n",
    "    df_input = df_input.drop('srch_co', 1)\n",
    "    #df_input = pd.merge(df_input, dest, on='srch_destination_id', how='left')\n",
    "    #df_input.fillna(-2.18, inplace=True)\n",
    "    #df_input = df_input.drop('srch_destination_id', 1)\n",
    "    print(\"pp part 2 complete\")\n",
    "    return df_input\n",
    "    \n",
    "data = addFeatures(data)\n",
    "test = addFeatures(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9417565, 18835131]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-be46f14e98d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mada\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madaest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mada\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\jk34678\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\jk34678\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0;32m--> 111\u001b[0;31m                          y_numeric=is_regressor(self))\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\jk34678\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\jk34678\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9417565, 18835131]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adaest = [2,5,10,15]\n",
    "data = data.iloc[:int(data.shape[0]/2)][:]\n",
    "data_y = data_y.iloc[:int(data_y.shape[0]/2)][:]\n",
    "for ada in adaest:\n",
    "    clf = AdaBoostClassifier(n_estimators=ada)\n",
    "    clf.fit(data.values, data_y.values)\n",
    "    probs = clf.predict_proba(test.values)\n",
    "    pred = [];\n",
    "    indices = [];\n",
    "    for i in range(len(test)):\n",
    "        pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "        indices.append(i)  \n",
    "\n",
    "    df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(\"result_12_8_16_Adaboost_\" + str(ada) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier()\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_MLP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_GNB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis() #singular value decomposition (default)\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_LDA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_QDA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SGD - logistic regression\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(loss='log',n_jobs=-1)\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_SGD_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SGD - SVM-like?\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(loss='modified_huber',n_jobs=-1) #sort of like SVM?\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_6_16_SGD_modified_huber.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DT Gini\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=25)\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_7_16_DT_gini_20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DT Entropy\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=25)\n",
    "clf.fit(data.values, data_y.values)\n",
    "probs = clf.predict_proba(test.values)\n",
    "pred = [];\n",
    "indices = [];\n",
    "for i in range(len(test.values)):\n",
    "    pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "    indices.append(i)  \n",
    "\n",
    "df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "df.to_csv(\"result_12_7_16_DT_entropy_20.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "est = [15, 20, 25, 30]\n",
    "for e in est:\n",
    "    clf = RandomForestClassifier(criterion = \"entropy\", n_estimators=e, max_depth=20,warm_start=True)\n",
    "    clf.fit(data.values, data_y.values)\n",
    "    probs = clf.predict_proba(test.values)\n",
    "\n",
    "    pred = [];\n",
    "    indices = [];\n",
    "    for i in range(len(test)):\n",
    "        pred.append(\" \".join(map(str, list(reversed(probs[i].argsort()[-5:])))))\n",
    "        indices.append(i)  \n",
    "\n",
    "    df = pd.DataFrame({\"id\" : indices, \"hotel_cluster\" : pred})\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(\"result_12_7_16_RF_20depth\" + str(e) + \"est_2014.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
